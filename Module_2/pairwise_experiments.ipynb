{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\", \"\"))\n",
    "\n",
    "class Toxicity(BaseModel):\n",
    "    toxicity: str = Field(description=\"\"\"'Toxic' if this the statement is toxic, 'Not toxic' if the statement is not toxic.\"\"\")\n",
    "\n",
    "def good_classifier(inputs: dict) -> dict:\n",
    "    prompt = f\"Classify the toxicity of the following statement as 'Toxic' or 'Not toxic':\\n{inputs['statement']}\"\n",
    "    model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "    resp = model.generate_content(prompt)\n",
    "    text = getattr(resp, \"text\", \"Not toxic\")\n",
    "    toxicity = \"Toxic\" if \"Toxic\" in text else \"Not toxic\"\n",
    "    return {\"class\": toxicity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a new task! Here, we have a salesperson named Bob. Bob has a lot of deals, so he wants to summarize what happened in this deals based off of some meeting transcripts.\n",
    "\n",
    "Bob is iterating on a few different prompts, that will give him nice, concise transcripts for his deals.\n",
    "\n",
    "Bob has curated a dataset of his deal transcripts, let's go ahead and load that in. You can take a look at the dataset as well if you're curious! Note that this is not a golden dataset, there is no reference output here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "  \"https://smith.langchain.com/public/9078d2f1-7bef-4ba7-b795-210a17682ef9/d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run some experiments on this dataset using two different prompts. Let's add an evaluator that tries to score how good our summaries are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\", os.getenv(\"GEMINI_API_KEY\", \"\")))\n",
    "\n",
    "SUMMARIZATION_SYSTEM_PROMPT = \"\"\"You are a judge, aiming to score how well a summary summarizes the content of a transcript.\n",
    "Return ONLY a JSON object with one field: {\"score\": <1-5 integer>}.\"\"\"\n",
    "\n",
    "SUMMARIZATION_HUMAN_PROMPT = \"\"\"\n",
    "[The Meeting Transcript] {transcript}\n",
    "[The Start of Summarization] {summary} [The End of Summarization]\n",
    "\"\"\"\n",
    "\n",
    "class SummarizationScore(BaseModel):\n",
    "    score: int = Field(\n",
    "        description=\"A score from 1-5 ranking how good the summarization is for the provided transcript, \"\n",
    "                    \"with 1 being a bad summary, and 5 being a great summary\"\n",
    "    )\n",
    "\n",
    "def summary_score_evaluator(inputs: dict, outputs: dict) -> dict:\n",
    "    model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "    prompt = (\n",
    "        SUMMARIZATION_SYSTEM_PROMPT\n",
    "        + \"\\n\\n\"\n",
    "        + SUMMARIZATION_HUMAN_PROMPT.format(\n",
    "            transcript=inputs[\"transcript\"],\n",
    "            summary=outputs.get(\"output\", \"N/A\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    resp = model.generate_content(prompt)\n",
    "\n",
    "    # Try parsing Gemini output into JSON\n",
    "    try:\n",
    "        data = json.loads(resp.text.strip())\n",
    "        summary_score = SummarizationScore(**data).score\n",
    "    except Exception:\n",
    "        # fallback if Gemini returns non-JSON text\n",
    "        summary_score = 1\n",
    "\n",
    "    return {\"key\": \"summary_score\", \"score\": summary_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run our experiment with a good version of our prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Good Summarizer - Gemini-7b0b35c5' at:\n",
      "https://smith.langchain.com/o/22e63b8c-9320-4dd3-b6d4-5404534c8e54/datasets/f20325c9-4169-409f-8051-51720c6ed484/compare?selectedSessions=2032c09d-4967-487f-bd3a-5b91bdda6d2d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd1390c4ef4484aa279c4fb1462c678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.transcript</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.summary_score</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob and Mr. Patel (CLOSED DEAL): Bob: Hello, M...</td>\n",
       "      <td>Mr. Patel met with Bob to find a hybrid midsiz...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5.341182</td>\n",
       "      <td>330f699b-e0c9-41cd-af6d-905393dd9b24</td>\n",
       "      <td>0aeba318-e633-4c6b-81bc-f40da35deb3e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob and Mr. Carter (CLOSED DEAL): Bob: Welcome...</td>\n",
       "      <td>Mr. Carter met with Bob to trade in his 2015 T...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>3.429572</td>\n",
       "      <td>4b5753e5-eda7-46ac-adbd-61ad91bf4907</td>\n",
       "      <td>551c8f43-ebd1-4ac2-b3fb-8622c80beff2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob and Ms. Nguyen (NO DEAL): Bob: Good aftern...</td>\n",
       "      <td>Ms. Nguyen sought a small, efficient city car,...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>5.049843</td>\n",
       "      <td>633d09af-f8dd-4ee1-ada3-19549bde4b0b</td>\n",
       "      <td>1c1cbb85-b3e4-445b-9097-414d95db0d65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob and Ms. Thompson (NO DEAL): Bob: Hi, Ms. T...</td>\n",
       "      <td>Bob, a Ford Motors salesman, met with Ms. Thom...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2.835680</td>\n",
       "      <td>8e3eba66-0661-436e-9a28-29eb267d4704</td>\n",
       "      <td>02e0010f-b9e4-4584-ac21-187735def2f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob and Mr. Johnson (CLOSED DEAL): Bob: Good m...</td>\n",
       "      <td>Mr. Johnson visited Ford Motors seeking a reli...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5.005380</td>\n",
       "      <td>9e06b9d9-4e08-438a-ac23-4b23fe3bd393</td>\n",
       "      <td>6cb8b519-54a2-46eb-85ee-72bd927a022a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Good Summarizer - Gemini-7b0b35c5>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\", os.getenv(\"GEMINI_API_KEY\", \"\")))\n",
    "\n",
    "# Prompt One: Good Prompt!\n",
    "def good_summarizer(inputs: dict):\n",
    "    model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "    prompt = (\n",
    "        f\"Concisely summarize this meeting in 3 sentences. \"\n",
    "        f\"Make sure to include all of the important events.\\n\\n\"\n",
    "        f\"Meeting: {inputs['transcript']}\"\n",
    "    )\n",
    "\n",
    "    resp = model.generate_content(prompt)\n",
    "\n",
    "    # Gemini returns text directly\n",
    "    return getattr(resp, \"text\", \"\").strip()\n",
    "\n",
    "# Run evaluation\n",
    "client.evaluate(\n",
    "    good_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],  # the Gemini version we built earlier\n",
    "    experiment_prefix=\"Good Summarizer - Gemini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll run an experiment with a worse version of our prompt, to highlight the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Bad Summarizer - Gemini-8bb9f08f' at:\n",
      "https://smith.langchain.com/o/22e63b8c-9320-4dd3-b6d4-5404534c8e54/datasets/f20325c9-4169-409f-8051-51720c6ed484/compare?selectedSessions=82a6ce05-d405-45d8-8edf-32e466001d50\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b0d6d032ff472484201e04fdd4d7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.transcript</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.summary_score</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob and Mr. Patel (CLOSED DEAL): Bob: Hello, M...</td>\n",
       "      <td>Bob successfully sells Mr. Patel a Ford Fusion...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>10.380007</td>\n",
       "      <td>330f699b-e0c9-41cd-af6d-905393dd9b24</td>\n",
       "      <td>6f999b74-617d-4ce8-a9a1-4b2fd014fa09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob and Mr. Carter (CLOSED DEAL): Bob: Welcome...</td>\n",
       "      <td>Bob successfully guides Mr. Carter through tra...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>8.237040</td>\n",
       "      <td>4b5753e5-eda7-46ac-adbd-61ad91bf4907</td>\n",
       "      <td>3bb1c44d-201f-48e1-bcbe-c07198ff8d17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob and Ms. Nguyen (NO DEAL): Bob: Good aftern...</td>\n",
       "      <td>Car salesman Bob failed to sell Ms. Nguyen a v...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>8.776305</td>\n",
       "      <td>633d09af-f8dd-4ee1-ada3-19549bde4b0b</td>\n",
       "      <td>84d0b95d-272c-45cd-bec1-5dab0b22fbe9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob and Ms. Thompson (NO DEAL): Bob: Hi, Ms. T...</td>\n",
       "      <td>Salesman Bob engaged Ms. Thompson, a browsing ...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>13.273849</td>\n",
       "      <td>8e3eba66-0661-436e-9a28-29eb267d4704</td>\n",
       "      <td>576007f2-6265-47cc-945a-8f578f3acdaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob and Mr. Johnson (CLOSED DEAL): Bob: Good m...</td>\n",
       "      <td>Salesperson Bob successfully sold a Ford Explo...</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>7.961925</td>\n",
       "      <td>9e06b9d9-4e08-438a-ac23-4b23fe3bd393</td>\n",
       "      <td>d899959e-e36c-461e-90bb-c72db2509d60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults Bad Summarizer - Gemini-8bb9f08f>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\", os.getenv(\"GEMINI_API_KEY\", \"\")))\n",
    "\n",
    "# Prompt Two: Worse Prompt!\n",
    "def bad_summarizer(inputs: dict):\n",
    "    model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "    prompt = f\"Summarize this in one sentence:\\n\\n{inputs['transcript']}\"\n",
    "\n",
    "    resp = model.generate_content(prompt)\n",
    "\n",
    "    # Gemini returns text directly\n",
    "    return getattr(resp, \"text\", \"\").strip()\n",
    "\n",
    "# Run evaluation\n",
    "client.evaluate(\n",
    "    bad_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],  # Gemini-based evaluator\n",
    "    experiment_prefix=\"Bad Summarizer - Gemini\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will compare our two experiments. These are the fields that pairwise evaluator functions get access to:\n",
    "- `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.\n",
    "- `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.\n",
    "- `reference_outputs: dict`: A dictionary of the reference outputs associated with the example, if available.\n",
    "- `runs: list[Run]`: A list of the full Run objects generated by the experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\n",
    "- `example: Example`: The full dataset Example, including the example inputs, outputs (if available), and metdata (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's give our LLM-as-Judge some instructions. In our case, we're just going to directly use LLM-as-judge to grade which of the summarizers is the most helpful.\n",
    "\n",
    "It might be hard to grade our summarizers without a ground truth reference, but here, comparing different prompts head to head will give us a sense of which is better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "Please act as an impartial judge and evaluate the quality of the summarizations provided by two AI summarizers to the meeting transcript below.\n",
    "Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their summarizations. \n",
    "Begin your evaluation by comparing the two summarizations and provide a short explanation. \n",
    "Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. \n",
    "Do not favor certain names of the assistants. \n",
    "Be as objective as possible. \"\"\"\n",
    "\n",
    "JUDGE_HUMAN_PROMPT = \"\"\"\n",
    "[The Meeting Transcript] {transcript}\n",
    "\n",
    "[The Start of Assistant A's Summarization] {answer_a} [The End of Assistant A's Summarization]\n",
    "\n",
    "[The Start of Assistant B's Summarization] {answer_b} [The End of Assistant B's Summarization]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function will take in an `inputs` dictionary, and a list of `outputs` dictionaries for the different experiments that we want to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=\"AIzaSyDaMP9Ay0peDYyempK1IJ0UhewtZs5QQ1E\")\n",
    "\n",
    "# Load Gemini model\n",
    "gemini_model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "class Preference(BaseModel):\n",
    "    preference: int = Field(description=\"\"\"1 if Assistant A answer is better based upon the factors above.\n",
    "2 if Assistant B answer is better based upon the factors above.\n",
    "Output 0 if it is a tie.\"\"\")\n",
    "\n",
    "import time, re\n",
    "\n",
    "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
    "    prompt = f\"\"\"\n",
    "    {JUDGE_SYSTEM_PROMPT}\n",
    "\n",
    "    Transcript: {inputs[\"transcript\"]}\n",
    "\n",
    "    Answer A: {outputs[0].get(\"output\", \"N/A\")}\n",
    "    Answer B: {outputs[1].get(\"output\", \"N/A\")}\n",
    "\n",
    "    Reply with ONLY one number: 1, 2, or 0.\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = gemini_model.generate_content(prompt)\n",
    "            raw = response.text.strip()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                time.sleep(60)   # wait for quota reset\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    match = re.search(r\"\\b[0-2]\\b\", raw)\n",
    "    preference_score = int(match.group()) if match else 0\n",
    "\n",
    "    return [1,0] if preference_score==1 else [0,1] if preference_score==2 else [0,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our pairwise experiment with `evaluate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/22e63b8c-9320-4dd3-b6d4-5404534c8e54/datasets/f20325c9-4169-409f-8051-51720c6ed484/compare?selectedSessions=2032c09d-4967-487f-bd3a-5b91bdda6d2d%2C82a6ce05-d405-45d8-8edf-32e466001d50&comparativeExperiment=d8deb176-fbf3-48d9-9a93-b238bb1d27ec\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52da9b4ee5c34cfdb153a286063e4ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<langsmith.evaluation._runner.ComparativeExperimentResults at 0x1e10b5e8c20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "# Compare Good vs Bad Summarizer experiments\n",
    "evaluate(\n",
    "    (\n",
    "        \"Good Summarizer - Gemini-7b0b35c5\",   # Replace with your actual Good Summarizer experiment name/ID\n",
    "        \"Bad Summarizer - Gemini-8bb9f08f\"     # Replace with your actual Bad Summarizer experiment name/ID\n",
    "    ),\n",
    "    evaluators=[ranked_preference],  # Uses preference ranking evaluator\n",
    "    experiment_prefix=\"Summarizer Comparison - Gemini\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-langsmith",
   "language": "python",
   "name": "venv-langsmith"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
