{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you set your environment variables, including your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing: true Project: nischala-personal-project\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Inline env (optional). Prefer .env in project root.\n",
    "os.environ.setdefault(\"LANGSMITH_TRACING\", \"true\")\n",
    "os.environ.setdefault(\"LANGSMITH_PROJECT\", \"Langsmith_intro\")  # personalize project name\n",
    "os.environ.setdefault(\"GEMINI_API_KEY\", os.getenv(\"GEMINI_API_KEY\", \"\"))\n",
    "os.environ.setdefault(\"LANGSMITH_API_KEY\", os.getenv(\"LANGSMITH_API_KEY\", \"\"))\n",
    "\n",
    "# Load from .env if present\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "print(\"Tracing:\", os.getenv(\"LANGSMITH_TRACING\"), \"Project:\", os.getenv(\"LANGSMITH_PROJECT\"))\n",
    "\n",
    "# Install Gemini client if needed\n",
    "try:\n",
    "    import google.generativeai as genai  # noqa: F401\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"google-generativeai\", \"-q\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can use a .env file\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing with @traceable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\n",
    "\n",
    "The decorator works by creating a run tree for you each time the function is called and inserting it within the current trace. The function inputs, name, and other information is then streamed to LangSmith. If the function raises an error or if it returns a response, that information is also added to the tree, and updates are patched to LangSmith so you can detect and diagnose sources of errors. This is all done on a background thread to avoid blocking your app's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import nest_asyncio\n",
    "from langsmith import traceable\n",
    "\n",
    "# Add the path where utils.py is located\n",
    "sys.path.append(\"module_0\")\n",
    "\n",
    "from utils import get_vector_db_retriever  # now it should work\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import nest_asyncio\n",
    "from langsmith import traceable\n",
    "from utils import get_vector_db_retriever\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Gemini config\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\", \"\"))\n",
    "\n",
    "MODEL_PROVIDER = \"google\"\n",
    "MODEL_NAME = \"models/gemini-2.5-flash\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"retriever\", metadata={\"owner\": \"Nischala\", \"vectordb\": \"sklearn\"})\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"llm\", metadata={\"ls_provider\": MODEL_PROVIDER, \"ls_model_name\": MODEL_NAME})\n",
    "def call_gemini(messages: List[dict]):\n",
    "    # Convert messages to single prompt\n",
    "    prompt = \"\\n\\n\".join([m[\"content\"] for m in messages])\n",
    "    model = genai.GenerativeModel(MODEL_NAME)\n",
    "    resp = model.generate_content(prompt)\n",
    "\n",
    "    # Minimal adapter to keep .choices[0].message.content API\n",
    "    class Obj:\n",
    "        class ChoicesMsg:\n",
    "            def __init__(self, content):\n",
    "                self.message = type(\"Msg\", (), {\"content\": content})\n",
    "        def __init__(self, text):\n",
    "            self.choices = [self.ChoicesMsg(text)]\n",
    "    return Obj(resp.text)\n",
    "\n",
    "@traceable(run_type=\"chain\", metadata={\"app_version\": APP_VERSION})\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"},\n",
    "    ]\n",
    "    return call_gemini(messages)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@traceable handles the RunTree lifecycle for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with the `@traceable` decorator, simply decorate any Python function with `@traceable`. You must also set the `LANGSMITH_TRACING` environment variable to 'true' and the `LANGSMITH_API_KEY` environment variable to your API key. This setup allows traces to be logged to LangSmith.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with the @traceable decorator?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith supports sending arbitrary metadata along with traces.\n",
    "\n",
    "Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from typing import List\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Gemini setup (make sure GEMINI_API_KEY is in your env)\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\", \"\"))\n",
    "\n",
    "@traceable(run_type=\"retriever\", metadata={\"vectordb\": \"sklearn\"})\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\", metadata={\"app_version\": APP_VERSION})\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"},\n",
    "    ]\n",
    "    return call_gemini(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\", metadata={\"ls_model_name\": MODEL_NAME, \"ls_provider\": MODEL_PROVIDER})\n",
    "def call_gemini(messages: List[dict], model: str = MODEL_NAME):\n",
    "    # Convert OpenAI-like messages into single Gemini prompt\n",
    "    prompt = \"\\n\\n\".join([m[\"content\"] for m in messages])\n",
    "    model = genai.GenerativeModel(MODEL_NAME)\n",
    "    resp = model.generate_content(prompt)\n",
    "\n",
    "    # Adapter to mimic OpenAI's .choices[0].message.content\n",
    "    class Obj:\n",
    "        class ChoicesMsg:\n",
    "            def __init__(self, content):\n",
    "                self.message = type(\"Msg\", (), {\"content\": content})\n",
    "        def __init__(self, text):\n",
    "            self.choices = [self.ChoicesMsg(text)]\n",
    "    return Obj(resp.text)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context shows how to customize the run name of a `traceable` function using `{ name: \"parentTraceable\" }`. You can add metadata to a `generateText` call by including it in the `experimental_telemetry.metadata` option. The context does not explicitly demonstrate how to pass generic metadata directly to the `traceable` function's configuration itself.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I add Metadata to a Run with @traceable?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add metadata at runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata can be added to traces dynamically at runtime. For AI SDK, you can include metadata within the `experimental_telemetry` field of a `generateText` call. For LangChain runnables, this can be achieved using invocation parameters, with the metadata then inherited by all child runnables.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I add metadata at runtime?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-langsmith",
   "language": "python",
   "name": "venv-langsmith"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
